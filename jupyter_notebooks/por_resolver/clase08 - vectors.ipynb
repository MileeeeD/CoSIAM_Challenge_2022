{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/logo_cosiam/LogoCoSIAM_COL.png' style='height:230px;  margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "# M√≥dulo 8 (Parte 2): De palabras a vectores\n",
    "\n",
    "- Feature Engineering\n",
    "- Representaci√≥n vectorial de textos\n",
    "- TF-IDF\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# En cap√≠tulos anteriores...\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/pipeline/pipeline2b.png' style='height:600px;'> </centeR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hoy\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/pipeline/pipeline3.png' style='height:600px;'> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üöÄ Hoy veremos...\n",
    "  \n",
    "- Repaso de Feature Engineering en Machine Learning \n",
    "- Representaci√≥n de datos en forma num√©rica\n",
    "- Espacio sem√°ntico vectorial\n",
    "\n",
    "\n",
    "- M√©todos de vectorizaci√≥n\n",
    "    - One-Hot Encoding \n",
    "    - Bag of Words\n",
    "    - Bag of N-Grams\n",
    "    - TF-IDF\n",
    "    - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Engineering ([1](https://developers.google.com/machine-learning/glossary)) ([2](https://github.com/omar-florez/AI_Dictionary_English_Spanish/blob/master/release/AI_Dictionary.pdf))\n",
    "\n",
    "<br><center><img src='../img/clase08/FEng1.jpg' style='height:500px;'></center>\n",
    "    \n",
    "- Es el proceso de usar el conocimiento del dominio para crear atributos que sirvan para entrenar un modelo de aprendizaje autom√°tico. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <i>\"Garbage in, garbage out\"</i> -- Malos atributos, malos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align='center'>\n",
    "    <img src='../img/clase08/FEng1.jpg' style='height:500px; float: left; margin: 0px 15px 15px 0px'>\n",
    "</div>\n",
    "\n",
    "- Podemos cambiar de un sistema cartesiano $(x,y)$ a un sistema polar $(r,\\theta)$ con una simple transformaci√≥n de coordenadas: $$r = \\sqrt{x^2 + y^2} \\Rightarrow \\theta = \\tan^{-1} \\left(\\dfrac{y}{x}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align='center'>\n",
    "    <img src='../img/clase08/FEng2.jpg' style='height:500px; float: left; margin: 0px 15px 15px 0px'>\n",
    "</div>\n",
    "\n",
    "- Ahora vemos que es f√°cil dividir el conjunto usando $r=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Otros ejemplos de Feature Engineering \n",
    "\n",
    "- Atributos categ√≥ricos-- Ejemplo: G√©nero, edad (cubetas), raza (variable ficticia)\n",
    "- Valores faltantes y valores at√≠picos\n",
    "- Normalizaci√≥n\n",
    "- Fechas\n",
    "- **Feature engineering para NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Engineering para NLP\n",
    "\n",
    "<img src='../img/clase08/FEngNLP.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representaci√≥n de datos en forma num√©rica\n",
    "\n",
    "**Ejemplo: Im√°genes**\n",
    "\n",
    "<center><img src='../img/clase08/komp.jpg'></center>\n",
    "\n",
    "- Una imagen es representada en un computador en la forma de una matriz donde cada $celda[i,j]$ representa el p√≠xel $i,j$ de la imagen.<br>\n",
    "\n",
    "- De manera similar, un video es una colecci√≥n de fotogramas, donde cada fotograma es una im√°gen. Por lo tanto, cualquier video puede ser representado como una colecci√≥n de matrices. <br>\n",
    "\n",
    "- (Des)afortunadamente, representar texto  de manera num√©rica no es tan sencillo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Espacio sem√°ntico vectorial\n",
    "\n",
    "- Las redes neuronales pueden hacer que las m√°quinas entiendan las analog√≠as como los humanos [Mikolov et al., 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 1.16 s, total: 22.6 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Useful functions\n",
    "def load_glove(filename):\n",
    "    dic = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            vec = line.split()\n",
    "            dic[vec[0]]= np.array(vec[1:], dtype=float)\n",
    "    return dic\n",
    "\n",
    "def analogies(gloves, x, y, z, n):\n",
    "    dif_1 = gloves[x] - gloves[y]\n",
    "    distances=[]\n",
    "    for key,val in gloves.items():\n",
    "        if z!=key:\n",
    "            dif_2 = gloves[z]-gloves[key]\n",
    "            distances.append((np.linalg.norm(dif_1-dif_2),key))\n",
    "    distances.sort()\n",
    "    return [d[1] for d in distances[0:n]]\n",
    "\n",
    "gloves = load_glove(\"../archivos/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def print_analogy():\n",
    "    print(\"Enter a word or 'x:y as z:'\")\n",
    "    cmd = input(\"> \")\n",
    "\n",
    "    while cmd!=None:\n",
    "        try:  \n",
    "            match = re.search(r'(\\w+):(\\w+) as (\\w+):', cmd)\n",
    "            x = match.group(1).lower()\n",
    "            y = match.group(2).lower()\n",
    "            z = match.group(3).lower()\n",
    "\n",
    "            words = analogies(gloves, x, y, z, 5)\n",
    "\n",
    "            print(\"%s is to %s as %s is to {%s}\" % (x,y,z,' '.join(words)))\n",
    "            cmd = input(\"> \")\n",
    "        except:\n",
    "            print(\"Bye mis cielas!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word or 'x:y as z:'\n",
      "> \n",
      "Bye mis cielas!\n"
     ]
    }
   ],
   "source": [
    "print_analogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¬øQu√© hay bajo el cap√≥?\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/brain.jpg' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Imaginemos que todas las palabras y sus significados viven en un espacio de altas dimensiones. Nosotros lo llam√°remos **espacio sem√°ntico vectorial**<br>\n",
    "\n",
    "- Cada dimensi√≥n en el espacio sem√°ntico vectorial representa alg√∫n aspecto del significado de la palabra\n",
    "\n",
    "- Los conceptos y las palabras que significan cosas similares deben vivir cerca en este espacio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¬øC√≥mo sabemos qu√© significa una palabra?\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/reina.jpg' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nosotros aprendemos a trav√©s de la experiencia\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/reyes2.png' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Las m√°quinas tambi√©n aprenden a trav√©s de la experiencia\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='../img/clase08/maquina1.png' style='height:500px; float: left; margin: 0px 15px 15px 0px'>\n",
    "    <img src='../img/clase08/maquina2.png' style='height:500px; float: left; margin: 0px 15px 15px 0px'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¬øQu√© hay bajo el cap√≥? Representaci√≥n num√©rica de textos\n",
    "\n",
    "<center><img src='../img/clase08/espacio.jpg' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "<center><big><b>¬°Matem√°ticas!</center></big></b>\n",
    "\n",
    "<br>\n",
    "\n",
    "- significado($man$) - significado($king$) + significado($queen$) = significado($woman$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representaci√≥n vectorial de textos\n",
    "\n",
    "- Existen varios m√©todos\n",
    "- Lo que diferencia un m√©todo del otro es qu√© tan bien captura las propiedades ling√º√≠sticas del texto que representa y la cantidad de espacio que ocupa en memoria\n",
    "<br><br>\n",
    "- **M√©todos m√°s populares (obsoletos)**:\n",
    "    - One-Hot Encoding \n",
    "    - Bag of Words (Bolsa de palabras)\n",
    "    - Bag of N-Grams (Bolsa de n-gramas)\n",
    "    - TF-IDF\n",
    "    - Word embeddings (word2vec)\n",
    "        - CBOW (Bolsa de palabras continua)\n",
    "        - SkipGram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " #### (Par√©ntesis)\n",
    " \n",
    "**Corpus ling√º√≠stico**: Conjunto amplio y estructurado de ejemplos reales de uso de la lengua.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## üõ†Ô∏è One-Hot Encoding\n",
    " \n",
    " Mapear cada palabra en el vocabulario del corpus de texto a una identificaci√≥n √∫nica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Ventajas\n",
    "- Es intuitivo y f√°cil de entender\n",
    "- La implementaci√≥n es directa\n",
    "\n",
    "#### Desventajas\n",
    "- Genera una matriz dispersa\n",
    "- El vector de cada frase no tiene un tama√±o constante\n",
    "- No tiene noci√≥n de similitud entre palabras\n",
    "- Problema de fuera de vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## üõ†Ô∏è Bag of Words (BoW) -- Bolsa de Palabras\n",
    " \n",
    "- Representar el texto como una bolsa de palabras (ignorando orden y contexto) \n",
    "- Si dos piezas de texto tienen casi las mismas palabras, entonces pertenecen a la misma bolsa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/clase08/bebememe.png' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scikit-learn \n",
    "\n",
    "- Es una biblioteca para aprendizaje autom√°tico de software libre para el lenguaje de programaci√≥n Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si no nos importa la frecuencia, `binary=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ventajas\n",
    "- Es intuitivo y f√°cil de entender\n",
    "- La implementaci√≥n es directa\n",
    "- El vector de cada frase tiene un tama√±o constante\n",
    "\n",
    "#### Desventajas\n",
    "- Genera una matriz dispersa (¬øsoluci√≥n?)\n",
    "- No tiene noci√≥n de similitud entre palabras\n",
    "- Problema de fuera de vocabulario\n",
    "- Se pierde el orden de la informaci√≥n\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "M√©todo bastante usado en la industria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## üõ†Ô∏è Bag of N-Grams (BoN) -- Bolsa de n-gramas\n",
    " <br>\n",
    " <center><img src='../img/clase08/NY.jpg' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hasta el momento solo hemos visto las palabras como unidades independientes\n",
    "- Con la bolsa de n-gramas capturamos un poco de contexto y orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ventajas\n",
    "- Captura alguna informaci√≥n sobre el contexto y orden\n",
    "\n",
    "#### Desventajas\n",
    "- Genera una matriz dispersa ¬°r√°pidamente!\n",
    "- Problema de fuera de vocabulario\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "[M√°s sobre n-gramas](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## üõ†Ô∏è TF-IDF\n",
    " \n",
    "- En los m√©todos que vimos en la clase pasada no hay ninguna noci√≥n de que algunas palabras del documento sean m√°s importantes que otras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- TF-IDF (Term Frequency, Inverse Document Frequency) se ocupa de este tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Busca cuantificar la importancia de una palabra relativa a las otras palabras del documento y del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Se usa frecuentemente en los sistemas de recuperaci√≥n de informaci√≥n y algoritmos de agrupaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Entre m√°s ayuda una palabra a distinguir un documento de los dem√°s, m√°s alta va a ser su puntuaci√≥n TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF: Term Frequency\n",
    "\n",
    "- **Frecuencia de t√©rminos**: Contar el n√∫mero de ocurrencias de una palabra en un documento, dividido por el n√∫mero de palabras en ese documento\n",
    "\n",
    "$$tf(t,d) = \\dfrac{count(t)}{|d|}$$\n",
    "\n",
    "- $t$ = t√©rmino\n",
    "- $d$ = documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üëÆ Pop Quiz\n",
    "\n",
    "- ¬øCu√°l es el valor m√°ximo de $tf(t,d$)?\n",
    "\n",
    "$$tf(t,d) = \\dfrac{count(t)}{|d|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DF: Document Frequency\n",
    "\n",
    "- La *frecuencia de t√©rminos* es m√°s alta para palabras frecuentemente usadas en un documento\n",
    "\n",
    "\n",
    "- **Frecuencia en documentos**: Es el n√∫mero de documentos que tienen esa palabra sobre el n√∫mero total de documentos\n",
    "\n",
    "$$df(t,N) = \\dfrac{|\\{d_i:t\\in d_i, i=1,\\cdots, N\\}|}{N}$$\n",
    "\n",
    "- $t$ = t√©rmino\n",
    "- $N$ = n√∫mero de documentos en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- La frecuencia en documentos es m√°s alta para palabras usadas en muchos documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Por otro lado, una palabra espec√≠fica a alg√∫n documento va a tener frecuencia de t√©rmino muy baja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Como el objetivo es distinguir un documento del otro, queremos resaltar las palabras usadas frecuentemente en un documento pero penalizarlas si est√°n presentes en todos los documentos. A esto se le llama la puntuaci√≥n **TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Inicialmente, \n",
    "$$tfidf(t,d,N) = \\dfrac{tf(t,d)}{df(t,N)}$$\n",
    "\n",
    "- Pero esto no nos da un buen puntaje. La f√≥rmula mejorada es:\n",
    "\n",
    "$$tfidf(t,d,N) = tf(t,d) \\cdot \\log\\left(\\dfrac{1}{df(t,N)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cuando $t$ est√° en todos los documentos, $idf$ es $\\log(1) = 0$\n",
    "\n",
    "- Esto tiene sentido ya que una palabra que est√° en todos los documentos es muy mala para distinguir entre documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/clase08/bebememe.png' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "M√°s info en [1](https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d), [2](https://github.com/parrt/msds692/blob/master/notes/tfidf.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Medidas de similitud\n",
    "\n",
    "¬øQu√© tan parecidos son los documentos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Medidas de similitud: Distancia euclidiana\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/dist_euc.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Medidas de similitud: Distancia del coseno\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/dist_cos.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ¬øCu√°ndo usar la distancia del coseno en vez de la euclidiana?\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/cosine.png' style='height:800px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # Pasando de TF-IDF a Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hasta el momento, las representaciones vectoriales de texto que hemos vistos tratan las unidades ling√º√≠sticas como unidades at√≥micas\n",
    "- Los vectores son dispersos\n",
    "- Tienen problema con palabras fuera del vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Word2Vec es frecuentemente llamado una representaci√≥n distribuida, mientras que TF-IDF, BoW, etc. son llamados representaciones locales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Con representaciones distribuidas, como *word2vec*, podemos crear representaciones densas y bajas en dimensi√≥n que capturan **similitudes distributivas** entre palabras y su **significado**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## üõ†Ô∏è Word2Vec\n",
    " \n",
    " - ¬øQu√© quiere decir que debemos capturar las **similitudes distributivas** entre palabras?\n",
    " - ¬øQu√© quiere decir el **significado** de una palabra?\n",
    " \n",
    "# üß† Los computadores son tan s√≥lo una extenci√≥n de nuestro cerebro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î ü§î ü§î\n",
    "# ¬øQu√© es tichiniky? \n",
    "# ü§î ü§î ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î ¬øQu√© es tichiniky? \n",
    "\n",
    "- Joe le ofreci√≥ a su novia una copa de <font color='red'>tichiniky</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Los platos de carne roja se hacen para complementar el <font color='red'>tichiniky</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Charlie se puso de pie tambale√°ndose, con la cara enrojecida por el exceso de <font color='red'>tichiniky</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Anoche cen√© pan, queso y este excelente <font color='red'>tichiniky</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Las bebidas estuvieron deliciosas: <font color='red'>tichiniky</font> rojo sangre, as√≠ como el dulce renano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î ¬øQu√© es tichiniky? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Una bebida alcoholica hecha de uvas y ma√≠z.\n",
    "<img src='../img/clase08/amor_amistad.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sem√°ntica distributiva\n",
    "\n",
    "- Una botella de <font color='red'>tichiniky</font> est√° en la mesa.\n",
    "- A todo el mundo le gusta el <font color='red'>tichiniky</font>.\n",
    "- No tomes <font color='red'>tichiniky</font> antes de conducir.\n",
    "- Hacemos <font color='red'>tichiniky</font> con uvas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sem√°ntica distributiva\n",
    "\n",
    "- Una botella de <font color='red'>____________</font> est√° en la mesa.\n",
    "- A todo el mundo le gusta el <font color='red'>____________</font>.\n",
    "- No tomes <font color='red'>____________</font> antes de conducir.\n",
    "- Hacemos <font color='red'>____________</font> con uvas. \n",
    "\n",
    "\n",
    "### ü§î ¬øQu√© otras palabras encajan en estos contextos? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sem√°ntica distributiva\n",
    "\n",
    "1. Una botella de ____________ est√° en la mesa.\n",
    "2. A todo el mundo le gusta el ____________.\n",
    "3. No tomes ____________ antes de conducir.\n",
    "4. Hacemos ____________ con uvas. \n",
    "\n",
    "\n",
    "|                 | 1 | 2 | 3 | 4 | ... |\n",
    "|-----------------|---|---|---|---|-----|\n",
    "| tichiniky        | 1 | 1 | 1 | 1 |     |\n",
    "| fuerte          | 0 | 0 | 1 | 0 |     |\n",
    "| aceite de motor | 1 | 0 | 1 | 0 |     |\n",
    "| tortillas       | 0 | 1 | 0 | 0 |     |\n",
    "| vino            | 1 | 1 | 1 | 1 |     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ‚ö†Ô∏è Tichiniky & Vino tienen la misma representaci√≥n de vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è ¬øC√≥mo funciona Word2Vec?\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/espacio.jpg' style='height:400px; float: center; margin: 0px 15px 15px 0px'>\n",
    "</center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Word2Vec deriva el significado de una palabra por su contexto. Es decir, si dos palabras diferentes ocurren en contextos parecidos, entonces es probable que signifiquen lo mismo\n",
    "\n",
    "\n",
    "- Recientemente,  [Mikolov et al., 2013] mostr√≥ que las redes neuronales hacen un buen trabajo representando el espacio sem√°ntico vectorial\n",
    "\n",
    "\n",
    "- Las representaciones de texto usando redes neuronales son por lo general llamadas <b><i>embeddings</b></i> ([encaje](https://es.wikipedia.org/wiki/Encaje_(matem%C3%A1tica)), inmersi√≥n √≥ incrustaci√≥n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è CBOW (Continous bag of words) \n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase08/cbow_fox.png' style='height:100px; float: center; margin: 0px 15px 15px 0px'>\n",
    "    Ventana contextual de tama√±o 2<br>\n",
    "    ¬øCu√°l es la probabilidad de que \"jumped\" ocurra dado que ...? $P(\\text{jumped}|...)$\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src='../img/clase08/cbow_prep.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src='../img/clase08/cbow_nn.png' style='height:800px; float: center; margin: 0px 15px 15px 0px'>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è SkipGram\n",
    "\n",
    "- Parecido a la bolsa de palabras continuas (`CBOW`), excepto que mientras que `CBOW` intenta predecir una palabra usando su contexto, el modelo `Skip-gram`, intenta predecir el contexto usando una palabra. ([M√°s info aqu√≠](https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314))\n",
    "\n",
    "<br><br>\n",
    "<center><img src='../img/clase08/sg_prep.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'>\n",
    "    Ventana contextual de tama√±o 2\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### üß† ¬øRed neuronal?\n",
    "\n",
    "\n",
    "- Inicializamos cada palabra $w$ en el corpus con un vector $v_w$ con valores aleatorios\n",
    "\n",
    "\n",
    "- Luego el modelo Word2Vec refina el vector $v_w$ al predecir $v_w$ usando los vectores de las palabras en su contexto usando una red neuronal de dos capas\n",
    "\n",
    "\n",
    "- Word2Vec se asegura de que estas representaciones sean de baja dimensi√≥n y densas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è Embeddings pre-entrenados\n",
    "\n",
    "- Entrenar tu propio embedding es un proceso muy costoso (en terminos computacionales y de tiempo)\n",
    "\n",
    "- Afortunadamente, existen embeddings pre-entrenados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üá¨üáß Ingl√©s: [Standford GloVe](https://nlp.stanford.edu/projects/glove/). Descarga [aqu√≠](http://nlp.stanford.edu/data/glove.6B.zip) \n",
    "\n",
    "- üá™üá∏ Espa√±ol: [DCC UChile](https://github.com/dccuchile/spanish-word-embeddings#word2vec-embeddings-from-sbwc). Descarga [aqu√≠](http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.txt.bz2)\n",
    "\n",
    "- üåé M√°s idiomas: [FastText](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "\n",
    "\n",
    "\n",
    "Jupyter notebook de referencia incluido en el material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/clase08/bebememe.png' style='height:500px; float: center; margin: 0px 15px 15px 0px'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Instalar Gensim\n",
    "\n",
    "Dado las recientes actualizaciones de **Gensim** por favor utilizar el siguiente comando en la consola:\n",
    "\n",
    "``conda install -c conda-forge gensim``\n",
    "\n",
    "Dependiendo de su instalaci√≥n, puede que el comando de arriba no le funcione. Por favor intente los siguientes:\n",
    "- ``pip install --upgrade gensim``\n",
    "- ``conda install -c anaconda gensim``\n",
    "- ``conda update all``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ü§ì Recapitulando: Hoy aprend√≠mos...\n",
    "\n",
    "- Herramientas espec√≠ficas de pre-procesamiento de texto en NLP\n",
    "    - Palabras vac√≠as\n",
    "    - Tokenizaci√≥n\n",
    "    - Stemming\n",
    "    - Lematizaci√≥n\n",
    "    - Etiquetado gramatical\n",
    "    \n",
    "    \n",
    "- Repaso de Feature Engineering en Machine Learning \n",
    "- Representaci√≥n de datos en forma num√©rica\n",
    "- Espacio sem√°ntico vectorial\n",
    "\n",
    "\n",
    "- M√©todos de vectorizaci√≥n\n",
    "    - One-Hot Encoding \n",
    "    - Bag of Words (Bolsa de palabras)\n",
    "    - Bag of N-Grams (Bolsa de n-gramas)\n",
    "    - TF-IDF\n",
    "    - Word embeddings (word2vec)\n",
    "        - CBOW (Bolsa de palabras continua)\n",
    "        - SkipGram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../img/dragonball/7.jpeg' style='height:600px; float: left; margin: 0px 15px 15px 0px'>\n",
    "\n",
    "‚Ä¢ **M√≥dulo 1**: &nbsp;&nbsp;&nbsp;&nbsp; Introducci√≥n ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 2**: &nbsp;&nbsp;&nbsp;&nbsp; Configuraci√≥n de ambiente de desarrollo ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 3**: &nbsp;&nbsp;&nbsp;&nbsp; Repaso de Python ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 4**: &nbsp;&nbsp;&nbsp;&nbsp; Panorama general del Aprendizaje Autom√°tico (Machine Learning) ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 5**: &nbsp;&nbsp;&nbsp;&nbsp; ¬øC√≥mo adquirir datos? ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 6**: &nbsp;&nbsp;&nbsp;&nbsp; Cuenta de desarrollador de Twitter ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 7**: &nbsp;&nbsp;&nbsp;&nbsp; Web Scraping ‚úÖ <br>\n",
    "‚Ä¢ **M√≥dulo 8**: &nbsp;&nbsp;&nbsp;&nbsp; De palabras a vectores ‚úÖ <br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
