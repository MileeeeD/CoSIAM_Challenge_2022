{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='../img/logo_cosiam/LogoCoSIAM_COL.png' style='height:230px;  margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "# Módulo 10: Redes neuronales\n",
    "\n",
    "- Introducción a las redes neuronales\n",
    "- Transformers con Hugging Face\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# En capítulos anteriores...\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/pipeline/pipeline5b.png' style='height:600px;'> </centeR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# En capítulos anteriores...\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/pipeline/pipeline5b.png' style='height:600px;'> </centeR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hoy: Regresemos a modelar\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/pipeline/pipeline6.png' style='height:600px;'> </centeR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué es NLP?\n",
    "\n",
    "<center><img src='../img/clase11/vector.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- El **Procesamiento de Lenguaje Natural** es una área de la Inteligencia Artificial que permite a los computadores entender, interpretar y usar el lenguaje humano. Es una herramienta que nos permite obtener información a partir del lenguaje humano (normalmente textos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<center><img src='../img/clase11/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1950: Alan Turing describió una \"máquina pensante\". Afirmó que si una máquina pudiera participar en una conversación e imitara a un humano de forma tan completa que no hubiera diferencias perceptibles, entonces la máquina podría considerarse capaz de pensar. <br><br>\n",
    "\n",
    "- 1952: El modelo Hodgkin-Huxley mostró cómo el cerebro utiliza las neuronas para formar una red eléctrica. <br><br>\n",
    "\n",
    "- ^ Eso inspiró la creación de AI & NLP & la evolución de los computadores. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<center><img src='../img/clase11/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1954: **Bag of Words**\n",
    "\n",
    "    - Cuenta la ocurrencia de palabras en un documento.\n",
    "    - Díficil de aplicar en la vida real porque rápidamente ocupa mucha memoria y es dominado por las palabras vacías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<center><img src='../img/clase11/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 1972: **TF-IDF**\n",
    "\n",
    "    - Eliminó el problema de las palabras vacías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<center><img src='../img/clase11/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 2013: **Word2Vec**\n",
    "\n",
    "    - Técnica que toma encuenta la relación semántica entre palabras\n",
    "    - De los primeros modelos que usaron técnicas de predicción y redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<center><img src='../img/clase11/evol.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- 2018: **Transformers**\n",
    "\n",
    "    - Modelos que usan atención para aumentar la velocidad del entrenamiento en tareas específicas. \n",
    "    \n",
    "**Recordemos** que NLP tiene muchas aplicaciones en la vida real que usan diferentes técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolución de NLP\n",
    "\n",
    "<br>\n",
    "<center><img src='../img/clase11/evolnnn.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "<br>\n",
    "\n",
    "- Encoder: ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa\n",
    "- Decoder: CTRL, GPT, GPT-2, Transformer XL\n",
    "- Seq2Seq: BART, mBART, Marian, T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 🧠 Redes Neuronales\n",
    "\n",
    "YouTube: https://www.youtube.com/watch?v=cQ54GDm1eL0&ab_channel=BuzzFeedVideo\n",
    "\n",
    "- Una red neuronal está compuesta de neuronas\n",
    "- Las **Redes Neuronales Artifciales** (ANN) están inspiradas las redes neuronales biológicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona biológica\n",
    "\n",
    "<center><img src='../img/clase11/neu.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "De una manera simplificada:\n",
    "- Las dendritas alimentan el cuerpo de la célula a través de señales eléctricas\n",
    "- La respuesta es después pasada a través del axón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "\n",
    "<center><img src='../img/clase11/perceptron.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "\n",
    "<center><img src='../img/clase11/p1.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Como todo modelo de *machine learning*, tenemos valores de entrada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "\n",
    "<center><img src='../img/clase11/p2b.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Después, los valores de entrada son multiplicados por pesos\n",
    "- Estos pesos son inicializados de manera aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "\n",
    "<center><img src='../img/clase11/p3.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- El resultado (en este ejemplo $12\\cdot.5 + 4\\cdot-1 = 2$) se pasa por una función de activación\n",
    "- Existen muchas funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "\n",
    "<center><img src='../img/clase11/p4.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- El resultado (en este ejemplo $12\\cdot.5 + 4\\cdot-1 = 2$) se pasa por una función de activación\n",
    "- Existen muchas funciones de activación: **EJEMPLO**: Positiva=1, Negativa=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una neurona artificial = Perceptrón \n",
    "<br>\n",
    "<center><img src='../img/clase11/p5.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Se agrega un sesgo para evitar problemas matemáticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una red neuronal artificial\n",
    "<br>\n",
    "<center><img src='../img/clase11/p5.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Matemáticamente tenemos:\n",
    "$$\\sum^{n}_{i=0}w_ix_i + b$$\n",
    "<br>\n",
    "- Cuando tenemos una red neuronal compuesta de varios perceptrones, se extiende a forma matricial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 Una red neuronal artificial\n",
    "<br>\n",
    "<center><img src='../img/clase11/ann.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "Partes:\n",
    "- Capa de entrada (input): Valores reales de los datos\n",
    "- Capas ocultas (2 en este caso): Deep network\n",
    "- Capa de salida (output): Estimado final\n",
    "\n",
    "A medida que crece el número de capas, crece el nivel de abstracción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 👮‍♀️ Punto de control\n",
    "<br>\n",
    "<center><img src='../img/clase11/control.png' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "### ¿Cuántas capas ocultas tiene esta red neuronal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 💥 Función de activación\n",
    "<br>\n",
    "<center><img src='../img/clase11/fa1.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Asigna 0 si el valor es menor a 0, asigna 1 si el valor es mayor o igual a cero\n",
    "- Muy drástica, pequeños cambios no son reflejados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 💥 Función de activación\n",
    "<br>\n",
    "<center><img src='../img/clase11/fa2.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Función sigmoide:** $f(x)=\\dfrac{1}{1+e^{-(x)}}$\n",
    "- Muy útil dependiendo de la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 💥 Función de activación\n",
    "<br>\n",
    "<center><img src='../img/clase11/fa3.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Función de la tangente hiperbólica:** $\\tanh x=\\dfrac{\\sinh x}{\\cosh x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 💥 Función de activación\n",
    "<br>\n",
    "<center><img src='../img/clase11/fa4.png' style='height:250px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- **Función ReLU** (Rectificador lineal): $max(0,x)$\n",
    "- Aunque es sencilla, tiene muy buen desempeño en muchas situaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 RNN: Redes Neuronales Recurrentes\n",
    "\n",
    "- Diseñadas para trabajar con datos de **secuencia **\n",
    "    - Series de tiempo\n",
    "    - Frases\n",
    "    - Audio\n",
    "    - Trayectoría de carros\n",
    "    - Música"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🧠 RNN: Redes Neuronales Recurrentes\n",
    "<br>\n",
    "<center><img src='../img/clase11/rnnvsann.png' style='height:450px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- Se envía el resultado a sí misma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🤔 RNN: Redes Neuronales Recurrentes\n",
    "\n",
    "- Un problema de las RNN es que empiezan a olvidar los primeros datos de entrada\n",
    "- Soluciones: \n",
    "    - LSTM (Long Short-Term Memory) \n",
    "    - GRU (Gated Recurrent Unit)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notebook de Redes Neuronales\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 🤗 Transformers con Hugging FAce\n",
    "<br>\n",
    "<center><img src='../img/clase11/trans.jpeg' style='height:300px; float: center; margin: 0px 15px 15px 0px'></center>\n",
    "\n",
    "- [Hugging Face](https://huggingface.co/) es una startup que ofrece 30 modulos pre-entrenados en más de 100 idiomas y 8 arquitecturas para NLU & NLG.\n",
    "\n",
    "    - BERT (from Google);\n",
    "    - GPT (from OpenAI);\n",
    "    - GPT-2 (from OpenAI);\n",
    "    - Transformer-XL (from Google/CMU);\n",
    "    - XLNet (from Google/CMU);\n",
    "    - XLM (from Facebook);\n",
    "    - RoBERTa (from Facebook);\n",
    "    - DistilBERT (from Hugging Face).\n",
    "    \n",
    "    \n",
    "- Recurso: https://www.kdnuggets.com/2021/02/hugging-face-transformer-basics.html\n",
    "- Recurso: https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Qué pueden hacer los transformers?\n",
    "\n",
    "- `pip install transformers`\n",
    "- Si se presentan problemas: `pip install datasets evaluate transformers[sentencepiece]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/coSIAM2022/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|███████████████████████████████████████| 629/629 [00:00<00:00, 195kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 268M/268M [00:05<00:00, 53.4MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 48.0/48.0 [00:00<00:00, 30.2kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 232k/232k [00:00<00:00, 4.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598051905632019},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Análisis de sentimiento en español\n",
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████| 841/841 [00:00<00:00, 405kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 440M/440M [00:09<00:00, 46.3MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████| 528/528 [00:00<00:00, 453kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 242k/242k [00:00<00:00, 8.04MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 481k/481k [00:00<00:00, 10.2MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 67.0/67.0 [00:00<00:00, 40.0kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████| 112/112 [00:00<00:00, 69.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model='finiteautomata/beto-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9984717965126038},\n",
       " {'label': 'NEG', 'score': 0.9993962049484253}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"Qué feliz estoy\", \"Tengo mucha tristeza\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████| 834/834 [00:00<00:00, 505kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 440M/440M [00:08<00:00, 53.2MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████| 528/528 [00:00<00:00, 311kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 242k/242k [00:00<00:00, 4.94MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████| 112/112 [00:00<00:00, 69.5kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Esta es una clase sobre la inteligencia artificial ',\n",
       " 'labels': ['educación', 'política', 'astrología'],\n",
       " 'scores': [0.39564433693885803, 0.34613901376724243, 0.25821664929389954]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model='Recognai/bert-base-spanish-wwm-cased-xnli')\n",
    "classifier(\n",
    "    \"Esta es una clase sobre la inteligencia artificial \",\n",
    "    candidate_labels=[\"educación\", \"astrología\", \"política\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████| 914/914 [00:00<00:00, 376kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 261M/261M [00:04<00:00, 56.1MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████| 115/115 [00:00<00:00, 67.1kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 840k/840k [00:00<00:00, 16.1MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 499k/499k [00:00<00:00, 11.2MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████| 262/262 [00:00<00:00, 157kB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/anaconda3/envs/coSIAM2022/lib/python3.9/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'En esta clase, vas a aprender los siguientes temas ígneos que te serán útiles a ti mismo, desde lo que te gustaría conocer de los mejores puntos de vista. \\n\\nEl amor\\n\\nDesde el primer momento, mi pasión por la literatura'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model='DeepESP/gpt2-spanish')\n",
    "generator(\"En esta clase, vas a aprender los siguientes temas \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Llenar los blancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████| 625/625 [00:00<00:00, 373kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 672M/672M [00:12<00:00, 54.8MB/s]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|████████████████████████████████████| 28.0/28.0 [00:00<00:00, 15.9kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 872k/872k [00:00<00:00, 4.70MB/s]\n",
      "Downloading: 100%|██████████████████████████████████| 1.72M/1.72M [00:00<00:00, 15.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0835035964846611,\n",
       "  'token': 60352,\n",
       "  'token_str': 'actuales',\n",
       "  'sequence': 'esta clase te va a ensenar todo sobre los modelos actuales.'},\n",
       " {'score': 0.05415772274136543,\n",
       "  'token': 55314,\n",
       "  'token_str': 'modernos',\n",
       "  'sequence': 'esta clase te va a ensenar todo sobre los modelos modernos.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model='bert-base-multilingual-uncased')\n",
    "unmasker(\"Esta clase te va a enseñar todo sobre los modelos [MASK].\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NER: Named Entity Recognition (Reconocimiento de entidades nombradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|███████████████████████████████████████| 998/998 [00:00<00:00, 644kB/s]\n",
      "Downloading: 100%|██████████████████████████████████| 1.33G/1.33G [00:24<00:00, 54.0MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 60.0/60.0 [00:00<00:00, 36.4kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 213k/213k [00:00<00:00, 6.17MB/s]\n",
      "/opt/anaconda3/envs/coSIAM2022/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9971403,\n",
       "  'word': 'Viviana',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99833703,\n",
       "  'word': 'Manhattan',\n",
       "  'start': 33,\n",
       "  'end': 42}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Viviana and I work in Manhattan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Responder preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|███████████████████████████████████████| 473/473 [00:00<00:00, 290kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 261M/261M [00:04<00:00, 56.8MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 29.0/29.0 [00:00<00:00, 18.1kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 213k/213k [00:00<00:00, 5.85MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 436k/436k [00:00<00:00, 8.35MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9975185990333557, 'start': 28, 'end': 37, 'answer': 'Manhattan'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Viviana and I in Manhattan\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading: 100%|██████████████████████████████████| 1.80k/1.80k [00:00<00:00, 1.09MB/s]\n",
      "Downloading: 100%|██████████████████████████████████| 1.22G/1.22G [00:21<00:00, 56.9MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 26.0/26.0 [00:00<00:00, 11.4kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 899k/899k [00:00<00:00, 13.5MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 456k/456k [00:00<00:00, 8.01MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The Apollo program, also known as Project Apollo, was the third U.S. human spaceflight program . The first manned flight of Apollo was in 1968 . It was followed by the two-man Project Gemini (1962-66) and the Apollo-Soyuz Test Project .'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "\"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\n",
    "First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\n",
    "Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\n",
    "Project Mercury was followed by the two-man Project Gemini (1962-66).\n",
    "The first manned flight of Apollo was in 1968.\n",
    "Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\n",
    "Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\n",
    "Apollo used Saturn family rockets as launch vehicles.\n",
    "Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████| 1.44k/1.44k [00:00<00:00, 880kB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 312M/312M [00:05<00:00, 54.5MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 826k/826k [00:00<00:00, 10.1MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 802k/802k [00:00<00:00, 11.9MB/s]\n",
      "Downloading: 100%|██████████████████████████████████| 1.59M/1.59M [00:00<00:00, 16.7MB/s]\n",
      "Downloading: 100%|████████████████████████████████████| 44.0/44.0 [00:00<00:00, 25.1kB/s]\n",
      "/opt/anaconda3/envs/coSIAM2022/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/opt/anaconda3/envs/coSIAM2022/lib/python3.9/site-packages/transformers/generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey, how are you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "src = \"es\"  # source language\n",
    "trg = \"en\"  # target language\n",
    "\n",
    "model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sample_text = \"Hola, ¿cómo estás?\"\n",
    "batch = tokenizer([sample_text], return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**batch)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Enlaces de referencia \n",
    "\n",
    "- https://huggingface.co/models\n",
    "- https://huggingface.co/docs/transformers/main/en/main_classes/pipelines\n",
    "- https://huggingface.co/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 🤓 Recapitulando: Hoy aprendímos...\n",
    "\n",
    "- Evolución de NLP\n",
    "- Panorama general sobre las redes neuronales\n",
    "- RNN (Red neuronal recurrente) para generar texto\n",
    "- Transformers con HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../img/dragonball/11.jpeg' style='height:600px; float: left; margin: 0px 15px 15px 0px'>\n",
    "\n",
    "• **Módulo 1**: &nbsp;&nbsp;&nbsp;&nbsp; Introducción ✅ <br>\n",
    "• **Módulo 2**: &nbsp;&nbsp;&nbsp;&nbsp; Configuración de ambiente de desarrollo ✅ <br>\n",
    "• **Módulo 3**: &nbsp;&nbsp;&nbsp;&nbsp; Repaso de Python ✅ <br>\n",
    "• **Módulo 4**: &nbsp;&nbsp;&nbsp;&nbsp; Panorama general del Aprendizaje Automático (Machine Learning) ✅ <br>\n",
    "• **Módulo 5**: &nbsp;&nbsp;&nbsp;&nbsp; ¿Cómo adquirir datos? ✅ <br>\n",
    "• **Módulo 6**: &nbsp;&nbsp;&nbsp;&nbsp; Cuenta de desarrollador de Twitter ✅ <br>\n",
    "• **Módulo 7**: &nbsp;&nbsp;&nbsp;&nbsp; Web Scraping ✅ <br>\n",
    "• **Módulo 8**: &nbsp;&nbsp;&nbsp;&nbsp; De palabras a vectores ✅ <br>\n",
    "• **Módulo 9**: &nbsp;&nbsp;&nbsp;&nbsp; Modelos de Machine Learning ✅ <br>\n",
    "• **Módulo 10**: &nbsp; Visualizaciones ✅<br>\n",
    "• **Módulo 11**: &nbsp; Introducción a redes neuronales & Hugging Face ✅<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¡Eso es todo sobre la introducción a la ciencia de datos a través del procesamiento de lenguaje natural!\n",
    "<br>\n",
    "<img src='../img/dragonball/all.jpeg' style='height:400px;'>\n",
    "\n",
    "¡Éxitos en el challenge!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
